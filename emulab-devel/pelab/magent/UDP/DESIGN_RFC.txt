/*
 * Copyright (c) 2006 University of Utah and the Flux Group.
 * 
 * {{{EMULAB-LICENSE
 * 
 * This file is part of the Emulab network testbed software.
 * 
 * This file is free software: you can redistribute it and/or modify it
 * under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or (at
 * your option) any later version.
 * 
 * This file is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public
 * License for more details.
 * 
 * You should have received a copy of the GNU Affero General Public License
 * along with this file.  If not, see <http://www.gnu.org/licenses/>.
 * 
 * }}}
 */
		Application Centric Internet Modelling - UDP
		--------------------------------------------

Node Setup:
-----------

Application is run on the Emulab nodes, and it is LD_PRELOADed with
libnetmon.

libnetmon on each Emulab node talks to application monitor using 
a Unix domain socket and sends information about the sendto() 
(and datagram connect() ) calls (intervals between sendto calls, as well as size
of data sent by each call ) made by the application on that node.

The application monitor sends this information to the corresponding management
agent running on a planetlab node.


How does it Work ? 
------------------

Eg: Let 'nodeA' and 'nodeB' be the hosts running the management agent on planetlab.
Let us assume that the UDP data transfer is one directional, from nodeA -> nodeB.

As of now, the management agent is programmed to send UDP traffic just as it was
sent by the application running in Emulab - No effort is made to control the rate
at which UDP packets are sent and no congestion control is used. For practical
reasons, to play nice with TCP and other traffic in the network between planet lab
nodes, some form of congestion control would probably be introduced in the
management agent in the future.

The management agent on nodeA sends UDP packets to nodeB. We intend to
replicate the sendto() calls being done by the application in Emulab, observe
the throughput and RTT achieved by these packets and feed those parameters
back to the monitor. The application in Emulab is not affected by the exact contents 
of the data being sent between the planetlab nodes.


So, we use a simple application level protocol between the management agents.
This protocol is implemented using the application level data of the UDP packets.

Protocol in the data packets:
-----------------------------
nodeA sends UDP packets to nodeB, embedding monotonically increasing sequence numbers
in the packets. 

Whenever nodeA sends a packet to nodeB, it captures the timestamp when the packet is
about to be put on the network. On receiving an acknowledgement from nodeB, again a 
timestamp is taken at the time of arrival of the acknowledgement. Half of the difference
between these two timestamps for a packet gives an approximate value of the RTT
for that packet - Or the one way delay.

Acknowledgements ( & redundancy ):
---------------------------------
nodeB echoes the packets back, indicating that the particular
sequence number packet was correctly received.

In each ACK packet, nodeB includes the sequence number of the packet
being acknowledged, as well as the sequence numbers of the last 3 packets
it has received. Including these extra sequence numbers in each ACK makes
the throughput calculations resilient to the loss of a small number of
( 3 consecutive ACK packets can be dropped without any effect on the
throughput calculation ) ACK packets on the reverse path.

The ACK packets also contain the timestamps at the receiver when the original
packets were received. The timestamp for the packet being ACKed is an
absolute value ( in micro seconds ), and the values for redundant ACKs are
relative to this absolute value.

As of now, the acknowledgement packets are of the same size as the received
data packets. If they are a fixed minimum size, then it could affect the RTT
calculations. If the packets sent from nodeA -> nodeB are say 1500 bytes long
and the ACK packets are about 50 bytes, then the transmission delay on the forward
path is going to be more than the transmission delay on the reverse path.

When one way delay is calculated as half of RTT, this causes the one way delay
estimate to be lower the actual value for the forward path ( if the ACKs are a fixed minimum size).


Minimum Delay:
-------------

The minimum of these one way delays is taken as having no queuing delay and hence
is considered as the sum of ( propagation + transmission + processing ) delays
in the forward path. Whenever the minimum delay value changes, an event message is
sent to the application monitor with the new value - and the dummy net pipe
corresponding to the connection between nodeA and nodeB is updated with that delay value
for the delay queue.


Dummy net Queue Size:
---------------------

We also keep track of the maximum one way delay. The difference between the maximum one
way delay and the minimum from above, gives the value of approximate maximum queuing delay
on the forward path. Whenever there is a new maximum queuing delay value, an event is sent to the
application monitor and the value is used to set the maximum queue size for the dummy net pipe.

Note: UDP packets sent by from nodeA may not always be the maximum sized packets, as is most
often the case with TCP. So, the minimum and maximum delay values need to be calculated ( ideally for
every packet size), keeping the maximum sized UDP packet in mind. The one way delay values are
thus scaled to represent maximum sized packets. ( we divide the one way delay by number of bytes in
the packet + overhead, and then multiply it by 1518 ).

Throughput: 
-----------

Throughput = (size of data being ACKed) / ( last_receiver_time_stamp - current_receiver_time_stamp )

Here, throughput is being calculated depending on when each packet was seen by the receiver. The
assumption is that there is a single bottleneck link in the network: and the packets seen
by the receiver will be spaced ( in time ) depending on the queueing & transmission delays
encountered by the packets at the bottleneck.

We assume that the forward and reverse paths are symmetric, in terms of the path
capacity and delay introduced - although this might not be true in some cases.

However, the spacing between the packets at the receiver need not be the same as that 
at the bottleneck.This spacing can become compressed(similar to TCP ACK compression) later on in 
the network ( at a fast link for example ),and it is possible for the packets to arrive 
immedietly after one another at the receiver.

The delays seen between the receiver in this case will be different than the spacing between
the packets after the bottleneck link and we will end up overestimating the throughput value.

The reason for this is that we fail to take queuing delay into account for closely spaced
packets arriving at the receiver. Since the clocks are not synchronized between the sender and
receiver, it is not possible to accurately calculate the one way delay at the receiver.

This compression of packets can affect the throughput calculated - currently we do not of
any way to avoid this potential problem. ( it does seem to occur infrequently, only on a few paths ).


Problems that might crop up:
----------------------------

1) We are using libpcap in the management agent on planet lab nodes. Due to
long delays in scheduling of the application ( & the limited BPF buffer size), 
this might result in some UDP packets being dropped by libpcap.

The alternative is to use SO_TIMESTAMP option provided for datagram sockets.
These timestamps are also as accurate as libpcap timestamps and are provided
for each received UDP packet by the socket layer. If this option is used,
then we can do away with libpcap.

However, SO_TIMESTAMP option implies that we are dependent on the UDP receive
socket buffer not getting full, while we are on planet lab ready queue.
If this buffer is full and packets are dropped by the kernel, then the application
simply does not know about the dropped packets.

Since libpcap allows us to capture only part of each packet ( we plan to capture only
the first 128 bytes ), its buffer can hold a larger number of packets than UDP
receive buffer can ( assuming MTU sized UDP packets ), and hence should be less
affected by the packet drop problem.

This is just a hypothesis and needs to be tested on planetlab.


2) Reordering of packets on the forward path & reordering of ACKs on the reverse path

We currently ignore the reordering of packets/ACKs on forward/reverse paths.

If this happens persistently and is perceived as a characteristic of some links,
then it might need to be addressed.

Optimizations:
--------------

1) The UdpClient application sends packets at a fixed rate specified at the
command line. In order to keep of the time, gettimeofday() can be used. But,
on planet lab using gettimeofday() may cause the program to be scheduled
out because it is a system call. Instead, the sending times are now
being calculated by reading the values of the TSC register. Now,
the deviation between when we wish to send a packet and we actually send
it is around 2 ms against the 10+ ms when using gettimeofday.

2) Since we capture and process all UDP packets using libpcap, there is no
need to make recvfrom() calls, either at the sender or at the receiver. Those
calls have now been removed and we will not incur the overhead of the system calls
and the time to copy the packets to user space. I ran the application in a controlled
emulab environment and did not observe any increase in CPU usage when all the received
UDP packets were being dropped by the kernel ( due to full socket receive buffer ).


Handling reordering on the forward path:
----------------------------------------

RTT and DevRTT are calculated for the packets sent out, by using EWMA (similar to TCP)
for each received acknowledgement packet.

We consider a packet to be lost if 

1) We did not receive an Ack for the packet and,
2) we received ACKs for 10 packets sent after it ( OR ) The current timestamp is greater
than the time at which the packet was sent + 10*( ewmaRTT + 4*ewmaDevRTT).

Note: The UdpServer application is not concerned with reordering and replies to all the packets that
it receives ( they can be out of order ). When an ACK has been received, UDP sensors detect
that the ACK corresponds to a reordered packet - if they have seen ACKs for packets which
had greater sequence numbers than this packet.

Averaging of throughput:
------------------------

Throughput is averaged over the last received 100 ACKs or ACKs received in the last 500ms.
The throughput value is a simple moving average. If any packet loss is detected, then the 
value is reported as authoritative, else it is a tentative value.

