Notes on baking the images. See boss:~stoller/genirack/racks ...

Each rack gets a backed image of the control node, and two baked
images for boss and ops. The control node image is discussed below,
but first lets talk about the boss/ops XEN VMs. 

Briefly, the VMs are initially created as a XEN based ElabInElab on
Utah's Emulab, experiment using an NS file tailored to the eventual
environment via a bunch of attribute variables. For example, take a
look at rack.ns in this directory.

The initial config lines turn on/off some Emulab features, but most
importantly causes the ProtoGeni subsystem to be configured in and the
packages loaded. The CONFIG_GENIRACK takes it further, running the
ProtoGeni initsite script, which will generate all of the PG
certificates and upload them to the Emulab website. When running in
Utah, the VMs will look just like they would if they were on the
remote network, except that we temporarily change things so that they
will actually boot on our network. Later just before we take the
snapshots of the VMs, we clean that stuff out; when they next boot,
they better be booting on the control node at the remote site!

The rest of the config variables are set according to the particulars
of the site, as told to us by the local site admin. EXCEPT for the
email list addresses; we change those later so that the new site admin
is not spammed.

For each site, we need to create a directory with three files in it:

    ips.txt       - The ip addresses for the control/ilo interfaces.
                    iLO interface addresses should be 10.249.249.[12345].
    ilo.xml       - The XML file of ilo info from HP
    variables.txt - Some extra config variables.

The variables file is a bunch of passwords, which are random strings I
generated by piping some bytes from /dev/random into md5 and taking a
substring. You can generate a new one of these files by running:

    ./genpswds.pl > variables.txt

The ip.txt file is specific to the site of course, and the XML file
comes from HP via email. Note that the ROUTEABLE_IPRANGE in the NS
file has to exclude the all of the IPs used, including the ones
in the ips.txt file. We also want to leave a few free for adding a few
nodes later.

Utah Emulab is going to function as a secondary for each rack's boss
named. So we need to add a secondary record to our named config file,
in BOTH the internal and external views. Edit /etc/namedb/named.conf,
but be sure to "co -l" the file (make sure there are no changes to
the file first; people forget). Look for the BBN rack version as
an example.

	# Secondary for foo.net
	zone "foo.net" {
		type slave;
		# IP of rack boss
		masters { XXX.XXX.XXX.XXX; };
		# Change this too; domain name
		file "slave/foo.net.INTERNAL.db";
	};
	# Replace 242.1.192 with reverse dotted subnet
	# Replace 129/25 with as needed for subnet.
	#  Upper half of /25 is 129/25
	#  Lower half of /25 is 0/25
	zone "129/25.242.1.192.in-addr.arpa" in {
		type slave;
		# IP of rack boss
		masters { XXX.XXX.XXX.XXX; };
		# Change this too; domain name
		file "slave/reverse-foo.net.db";
	};

And then again for the external view (note the different file!):

	# Secondary for foo.net
	zone "foo.net" {
		type slave;
		# IP of rack boss
		masters { XXX.XXX.XXX.XXX; };
		# Change this too; domain name
		file "slave/foo.net.db";
	};
	# Replace 242.1.192 with reverse dotted subnet
	# Replace 129/25 with as needed for subnet.
	#  Upper half of /25 is 129/25
	#  Lower half of /25 is 0/25
	zone "129/25.242.1.192.in-addr.arpa" in {
		type slave;
		# IP of rack boss
		masters { XXX.XXX.XXX.XXX; };
		# Change this too; domain name
		file "slave/reverse-foo.net.db";
	};

Utah Emulab is also the DNS server for the control node IPs. So in
/etc/namedb/instageni.net.db we need two entries. For example:

	gpolab.control-nodes		IN	A	192.1.242.130
	gpolab-ilo.control-nodes	IN	A	192.1.242.131

Be sure to change the serial number at the top of the file.

ALSO: need to add new boss/ops ips to boss:/etc/namedb/named.conf
so that they can do recursive lookups. Look for the 'rd-allowed'
section.

Now run named_setup to get named restarted on boss. Be sure to tail
/var/log/messages to make sure no problems in the restart. 

Swap the experiment in.

Once the VMs are ready, copy the directory mentioned above over to the
inner boss as /usr/testbed/etc/genirack. Do NOT put this stuff on the
inner ops!

	boss> sudo scp -rp ~stoller/genirack/racks/XXX
	         	pcvmXXX-1:/usr/testbed/etc/genirack

Now we convert the VMs for boot in the target environment, which means
cleaning up some stuff and changing a bunch of things.  First ssh (as
root) into the inner boss VM from outer boss: Oh, use the "script"
command to save the errors.

	boss> cd /usr/testbed/obj/testbed/install
	boss> script
	boss> sudo perl emulab-install -b -i boss/genirack boss
	boss> exit

Grab (inner) boss:/usr/testbed/etc/elabman.pswd and store it in our
file. See Utah (outer) boss:~stoller/genirack/racks/pswd.txt.

*** New: Grab /usr/testbed/etc/{emulab.pem,openvpn-client.pem} and put
    them in the rack subdir (in a "vpn" subdir please) on outer boss.
    This is what we need to connect to the VPN (openvpn) that will be
    running on the control node, that allows you to access the ilo
    managment interfaces remotely.

Now ssh (as root) into inner ops:

	ops> cd /usr/testbed/obj/testbed/install
	ops> script
	ops> sudo perl emulab-install -b -i ops/genirack ops
	ops> exit

Now we have to shutdown the VMs. Log out of the boss/ops vms, and log
into the physical host and then:

	vhost-0> sudo /usr/local/etc/emulab/vnodesetup -jh pcvmXXX-1
	vhost-0> sudo /usr/local/etc/emulab/vnodesetup -jh pcvmXXX-2
	
The -h option is very important; it says to keep the disks intact.
If you forget that, you have to go back to the beginning and start
over.

NOTE: pcvmXXX-2 might take a while to actually shutdown cause boss
      is now gone. Do and "sudo xm list" to see when it really stops.

Next step is to capture the entire state of the VMs, which includes an
imagezip of each lvm, a copy of the kernel, and a slightly modified
xm.conf file:

	vhost-0> sudo /usr/local/etc/emulab/capturevm.pl -r boss pcvmXXX-1
	vhost-0> sudo /usr/local/etc/emulab/capturevm.pl -r ops pcvmXXX-2

This will take a little while of course. When finished, cd into
/capture and you will find two directories named boss and ops.  You
want to create a tar file (no point in using compression).

	vhost-0> sudo tar cf foo.tar boss ops

You will copy the tarfile over to the new control node after you
create the 4th partition on it (so there is enough space). So just
leave the tar file where it is for now. 

----

Control Node Image:

The control node image is currently baked from the Utah control
node. We have an extra disk on our control node that is a duplicate of
the main disk. Well, it was at one time, but we don't change it very
often and when I do, I try to remember to update the mirror as well.
Anyway, there are just a few things that need to be changed on the
control image for each site.

To log into the control node, load elabman's private key into your ssh
agent: /root/.ssh/elabman_dsa ... the password is stored in
boss:/usr/testbed/etc/elabman_dsa.pswd ... then ssh over to
elabman@utah.control.geniracks.net

You will need to create a little text file that provides all of the
details. See the ctrlvars.txt file in this directory. You will also
need the ssh pubkey file from the site admin. For example:

	address=128.112.170.2
	netmask=255.255.255.0
	gateway=128.112.170.1
	domain="instageni.cs.princeton.edu"
	forwarders="128.112.136.10,128.112.136.12"
	hostname="princeton.control-nodes.geniracks.net"
	timezone='America/New_York'
	rootpswd="cleverpswd"
	adminuser="acb"

*** Be sure to write down the root password in the pswd.txt file!

Copy this file and ssh pubkey file over to /tmp on Utah's control
node. Then log in and do this:

	control> sudo /usr/local/bin/bakectrl.pl -s /tmp/foo.txt /tmp/foo.pub

The -s option says to unmount the disk, create a frisbee image in
/scratch, and then remount the disk. Copy the file from /scratch over
to boss.emulab.net:/usr/testbed/www/downloads

Might be a good idea to delete the file from /tmp since it has a
password in it. But make sure you have stored a copy in the geniracks
directory to be safe.


